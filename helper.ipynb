{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import community as comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_node_label(filename, skip_head=False):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        if skip_head:\n",
    "            fin.readline()\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n",
    "    \n",
    "def split_train_evaluate(X, Y, embeddings, train_precent, seed=0):\n",
    "    state = np.random.get_state()\n",
    "    training_size = int(train_precent * len(X))\n",
    "    np.random.seed(seed)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "    X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "    Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "    X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "    Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "    # train\n",
    "    binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "    binarizer.fit(Y)\n",
    "    X_tr = [embeddings[x] for x in X_train]\n",
    "    Y_tr = binarizer.transform(Y_train)\n",
    "    clf=TopKRanker(LogisticRegression())\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    np.random.set_state(state)\n",
    "    top_k_list = [len(l) for l in Y_test]\n",
    "    X_test = np.asarray([embeddings[x] for x in X_test])\n",
    "    Y_pred = clf.predict(X_test, top_k_list=top_k_list)\n",
    "    results = {}\n",
    "    results['acc'] = accuracy_score(binarizer.transform(Y_test),Y_pred)\n",
    "    print('-------------------')\n",
    "    print(results)\n",
    "    print('-------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# healpers for node2vec\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "            (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random()*N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n",
    "    \n",
    "def get_alias_edge(G, p, q, t, v):\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight/p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(G, walks, embed_size=128, window_size=5, workers=3, iter=5, **kwargs):\n",
    "    kwargs[\"sentences\"] = walks\n",
    "    kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "    kwargs[\"vector_size\"] = embed_size\n",
    "    kwargs[\"sg\"] = 1  # skip gram\n",
    "    kwargs[\"hs\"] = 1  # deepwalk use Hierarchical Softmax\n",
    "    kwargs[\"workers\"] = workers\n",
    "    kwargs[\"window\"] = window_size\n",
    "    kwargs[\"epochs\"] = iter\n",
    "\n",
    "    print(\"Learning embedding vectors...\")\n",
    "    model = Word2Vec(**kwargs)\n",
    "    print(\"Learning embedding vectors done!\")\n",
    "\n",
    "    embeddings = {}\n",
    "    for word in G.nodes():\n",
    "        embeddings[word] = model.wv[word]\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepwalk_walks(G, num_walks, walk_length,):\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        walk.append(random.choice(cur_nbrs))\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    \n",
    "def node2vec_walks(G, p, q, num_walks, walk_length,):\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "\n",
    "        for edge in G.edges():\n",
    "            alias_edges[edge] = get_alias_edge(G, p, q, edge[0], edge[1])\n",
    "        \n",
    "        # generate walks\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        if len(walk) == 1:\n",
    "                            walk.append(\n",
    "                                cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                        else:\n",
    "                            prev = walk[-2]\n",
    "                            edge = (prev, cur)\n",
    "                            next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                            walk.append(next_node)\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_classification(embeddings, label):\n",
    "    X, Y = read_node_label(label,skip_head=True)\n",
    "    \n",
    "    ltrainfrac = [.7,]\n",
    "    for tf in ltrainfrac:\n",
    "        print(\"Training classifier using {:.2f}% nodes...\".format(tf * 100))\n",
    "        split_train_evaluate(X, Y, embeddings, tf)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def makeLinkPredictionData(graph, embeddings):\n",
    "    # converting embedding to a numpy array\n",
    "    X = [[0] for i in range(G.number_of_nodes())]\n",
    "    for i in range(0, G.number_of_nodes()):\n",
    "        X[i] = embeddings[str(i+1)]\n",
    "    X = np.array(X)\n",
    "    \n",
    "    Xd = []\n",
    "    Yd = []\n",
    "    count = 0\n",
    "    # for all vertices\n",
    "    for u in range(graph.number_of_nodes()):\n",
    "        Nu = list(graph.neighbors(u))\n",
    "        count += len(Nu)\n",
    "        cn = 0\n",
    "        totalns = 0\n",
    "        # for all neighbors of u\n",
    "        for n in Nu:\n",
    "            x = []\n",
    "            if n > u:\n",
    "                for d in range(len(X[0])):\n",
    "                    x.append(X[u][d] - X[n][d]) # distance between the embeddings of u and its neighbor n\n",
    "                Xd.append(x)\n",
    "                Yd.append(1) # positive sample (edge present)\n",
    "                totalns += 1\n",
    "        tmpnn = []\n",
    "        if len(Nu) > graph.number_of_nodes() // 2:\n",
    "            totalns = (graph.number_of_nodes() - len(Nu)) // 2\n",
    "            #print(\"Testing neighbors!\")\n",
    "        while cn < totalns:\n",
    "            nn = random.randint(0, graph.number_of_nodes() - 1)\n",
    "            # non-neighbors of u\n",
    "            if nn not in Nu and nn not in tmpnn:\n",
    "                cn += 1\n",
    "                x = []\n",
    "                for d in range(len(X[0])):\n",
    "                    x.append(X[u][d] - X[nn][d])\n",
    "                Xd.append(x)\n",
    "                Yd.append(0) # negative sample (edge absent)\n",
    "                tmpnn.append(nn)\n",
    "    Xd, Yd = np.array(Xd), np.array(Yd)\n",
    "    indices = np.array(range(len(Yd)))\n",
    "    np.random.shuffle(indices)\n",
    "    Xt = Xd[indices]\n",
    "    Yt = Yd[indices]\n",
    "    #print(len(Xd), len(Yd), count)\n",
    "    \n",
    "    \n",
    "    ltrainfrac = [0.05, 0.1, 0.2, 0.3, .4, .5, .6, .7, .8]\n",
    "    for tf in ltrainfrac:\n",
    "        CV = int(len(Yt) * tf)\n",
    "        trainX = Xt[0:CV]\n",
    "        testX = Xt[CV:]\n",
    "        trainY = Yt[0:CV]\n",
    "        testY = Yt[CV:]\n",
    "        modelLR = LogisticRegression().fit(trainX, trainY)\n",
    "        predictedY = modelLR.predict(testX)\n",
    "        acc = accuracy_score(predictedY, testY)\n",
    "        #f1macro = f1_score(predictedY, testY, average='macro', labels=np.unique(predictedY))\n",
    "        #f1micro = f1_score(predictedY, testY, average='micro', labels=np.unique(predictedY))\n",
    "        #print(\"Link predictions:\", tf, \":Accuracy:\",acc, \"F1-macro:\", f1macro, \"F1-micro:\",f1micro)\n",
    "        print(\"Link predictions:\", tf, \":Accuracy:\",acc)\n",
    "\n",
    "\n",
    "\n",
    "def cluster_eval(G, embeddings):\n",
    "    # converting embedding to a numpy array\n",
    "    X = [[0] for i in range(G.number_of_nodes())]\n",
    "    for i in range(0, G.number_of_nodes()):\n",
    "        X[i] = embeddings[str(i+1)]\n",
    "    X = np.array(X)\n",
    "\n",
    "    bestModularity = 0\n",
    "    bestC = 2\n",
    "    NOC = 30\n",
    "    allmodularity = []\n",
    "    for cls in range(2, NOC):\n",
    "        \n",
    "        # find clusters using a kmeans clustering algorithm on the embedding\n",
    "        # Number of clusters is set to cls\n",
    "        clusters = KMeans(n_clusters=cls, random_state=0).fit(X)\n",
    "        predG = dict()\n",
    "        for node in range(len(clusters.labels_)):\n",
    "            predG[node] = clusters.labels_[node]\n",
    "        \n",
    "        # compute the modularity score of the Kmeans clustering\n",
    "        modularity = comm.community_louvain.modularity(predG, G)\n",
    "        allmodularity.append(modularity)\n",
    "        print(\"Number of clusters: \", cls, \"  Modularity: \", modularity)\n",
    "        if modularity > bestModularity:\n",
    "            bestModularity = modularity\n",
    "            bestC = cls\n",
    "    plt.scatter(range(2, NOC), allmodularity)\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"Modularity score\")\n",
    "    plt.show()\n",
    "    #print(\"Best Modularity:\",bestModularity, \"Clusters:\", bestC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings, label):\n",
    "\n",
    "    X, Y = read_node_label(label,skip_head=True)\n",
    "    emb_list = []\n",
    "    for k in X:\n",
    "        emb_list.append(embeddings[k])\n",
    "    emb_list = np.array(emb_list)\n",
    "\n",
    "    model = TSNE(n_components=2)\n",
    "    node_pos = model.fit_transform(emb_list)\n",
    "    color_idx = {}\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        color_idx.setdefault(Y[i][0], [])\n",
    "        color_idx[Y[i][0]].append(i)\n",
    "\n",
    "    for c, idx in color_idx.items():\n",
    "        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)  # c=node_colors)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
